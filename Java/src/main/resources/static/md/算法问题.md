# 算法

> 逻辑回归解决了线性回归到线性分类的难题，而支持向量机解决了线性分类到非线性分类的难题。支持向量机就是想通过数学的方法，看看怎么样能够将数据“最大程度”地分离开，就要找这个“最大程度”，我们叫“边际”(margin)最大。而支持向量机的名字也是有原因的，“边际”其实只由两类中离分割线最近的点所决定，这些点叫“支持向量”。

> 部分内容转自cnblog刘建平pinard

## 1.机器学习算法

### 0.一些不知道放哪里的名词解释

1. **exposure bias**：在连环步骤问题里面，常常有1->2->3->4->……但假使中间有一个过程出现问题，那么这个过程就无法进行下去。比如我们为了寻找最优解，通常在第一层找最优，然后第二层，第三层……但如果有一个层出现了问题，那么后来的问题都会出错，所以会在某一布中遍历几种选择，从后面的结果去看前面无法确定的步骤，或者在每层中多做出几个选择

2. **预训练的意义**：

   ![预训练意义](D:\programming\typora_Image\预训练意义.jpg)

3. HMM(隐马尔科夫模型)

   这个模型的每个状态都只依赖于前一个的状态，这个假设被称为**马尔科夫假设**，但这个假设也会造成很多信息的丢失

   马尔可夫过程：该过程中，每个状态的转移只依赖于之前的 n 个状态，这个过程被称为1个 n 阶的模型，其中 n 是影响转移状态的数目。这和确定性系统不一样，因为这种转移是有概率的，而不是确定性的。

### 1.优化问题

> 优化损失函数

Url：https://www.cnblogs.com/pinard/category/894692.html

==**梯度消失：**==反向传播算法中，使用矩阵求导的链式法则，有很多连乘，如果连乘数字每层都小于1，梯度越往前就会越小，最终导致梯度消失

==**梯度爆炸：**==如果每层数字都大于1，则梯度越来越大，最终梯度爆炸

example:![img](https://upload-images.jianshu.io/upload_images/4749583-b5244660c8be2a8a?imageMogr2/auto-orient/strip|imageView2/2/w/652)

其中，b为偏置，w为权重，C是Loss函数，每一层的输出为a，$a=\sigma(z)=\sigma(w_1a_0+b_1)$

$z_j=w_j*a_{j-1}+b_j$，则第一个神经元的梯度为：

$\frac{\partial C}{\partial b_1}=\dot\sigma(z_1)*w_2*\dot\sigma(z_2)*w_3*\dot\sigma(z_3)*w_4*\dot\sigma(z_4)*\frac{\partial C}{\partial a_4}$

sigmoid函数的导数图像为<img src="https://upload-images.jianshu.io/upload_images/4749583-849b4ba1402e41c5?imageMogr2/auto-orient/strip|imageView2/2/w/665" alt="img" style="zoom:33%;" />

如果使用标准方法来初始化权重，w会变成一个0~1分布，|w|<1，则有$w_j*\dot\sigma(z)<\frac{1}{4}$，这些项连乘，结果会成指数级下降（如果初始权重w很大，此时b为一个合适值，也会得到爆炸的梯度）

==梯度消失、爆炸问题解决方法==：预训练+微调、梯度剪切（主要针对梯度爆炸，设置剪切阈值，如果超过阈值，则强制限制在阈值范围内）、权重正则化、激活函数导数为1（比如用relu、leakrelu、elu等）、BatchNorm（就是为了解决梯度问题提出）、残差结构（Resnet）、LSTM（门结构帮助记录信息）

> relu函数<img src="https://bkimg.cdn.bcebos.com/pic/024f78f0f736afc3d3cc48b4bf19ebc4b645121a?x-bce-process=image/watermark,image_d2F0ZXIvYmFpa2U4MA==,g_7,xp_5,yp_5" alt="img" style="zoom:50%;" />但也会导致在<0时，神经元无法激活，且输出不是以0为中心，

#### 1.梯度下降法

梯度：比如函数f(x,y), 分别对x,y求偏导数，求得的梯度向量就是(∂f/∂x, ∂f/∂y)^T^,简称grad f(x,y)或者▽f(x,y)。从几何意义上讲，就是函数变化增加最快的地方。沿着梯度向量的方向，更加容易找到函数的最大值。反过来说，沿着梯度向量相反的方向，也就是 -(∂f/∂x0, ∂f/∂y0)^T^的方向，梯度减少最快，也就是更加容易找到函数的最小值。

梯度下降法只能找到局部最优解

> 1.算法的步长选择。在前面的算法描述中，我提到取步长为1，但是实际上取值取决于数据样本，可以多取一些值，从大到小，分别运行算法，看看迭代效果，如果损失函数在变小，说明取值有效，否则要增大步长。前面说了。步长太大，会导致迭代过快，甚至有可能错过最优解。步长太小，迭代速度太慢，很长时间算法都不能结束。所以算法的步长需要多次运行后才能得到一个较为优的值。
>
> 2.算法参数的初始值选择。 初始值不同，获得的最小值也有可能不同，因此梯度下降求得的只是局部最小值；当然如果损失函数是凸函数则一定是最优解。由于有局部最优解的风险，需要多次用不同初始值运行算法，关键损失函数的最小值，选择损失函数最小化的初值。
>
> 3.归一化。由于样本不同特征的取值范围不一样，可能导致迭代很慢，为了减少特征取值的影响，可以对特征数据归一化，也就是对于每个特征x，求出它的期望$$ x - \overline{x} $$和标准差std(x)，然后转化为：
> $$
> \frac{x-\overline{x}}{sdt(x)}
> $$
> 这样特征的新期望为0，新方差为1，迭代速度可以大大加快。

##### 1.批量梯度下降法

更新参数时使用所有的样本来更新

##### 2.随机梯度下降法

仅仅选取一个样本来求梯度

##### 3.小批量梯度下降法

选取一部分样本来迭代，==业界最常用==



#### 2.最小二乘法

​	首先，最小二乘法需要计算X^T^X的逆矩阵，有可能它的逆矩阵不存在，这样就没有办法直接用最小二乘法了，此时梯度下降法仍然可以使用。当然，我们可以通过对样本数据进行整理，去掉冗余特征。让X^T^X的行列式不为0，然后继续使用最小二乘法。

​	第二，当样本特征n非常的大的时候，计算X^T^X的逆矩阵是一个非常耗时的工作（n*n的矩阵求逆），甚至不可行。此时以梯度下降为代表的迭代法仍然可以使用。那这个n到底多大就不适合最小二乘法呢？如果你没有很多的分布式大数据计算资源，建议超过10000个特征就用迭代法吧。或者通过主成分分析降低特征的维度后再用最小二乘法。

​	第三，如果拟合函数不是线性的，这时无法使用最小二乘法，需要通过一些技巧转化为线性才能使用，此时梯度下降仍然可以用。

​	第四，讲一些特殊情况。当样本量m很少，小于特征数n的时候，这时拟合方程是欠定的，常用的优化方法都无法去拟合数据。当样本量m等于特征数n的时候，用方程组求解就可以了。当m大于n时，拟合方程是超定的，也就是我们常用与最小二乘法的场景了。

#### 3.过拟合

> 部分转自 知乎 https://zhuanlan.zhihu.com/p/75207641?utm_source=wechat_session

==解决方法：==

在深度学习中，为了避免出现过拟合（Overfitting），通常输入充足的数据量是最好的解决办法。当数据无法达到模型的要求或者添加数据后模型由于某类数据过多导致过拟合时，以下方法也可以发挥一些作用：

**(1)、Regularization**：数据量比较小会导致模型过拟合, 使得训练误差很小而测试误差特别大. 通过在Loss Function 后面**加上正则项**可以抑制过拟合的产生。缺点是引入了一个需要手动调整的hyper-parameter。

**(2)、Dropout**：这也是一种正则化手段，不过跟以上不同的是它通过随机将部分神经元的输出置零来实现。

**(3)、Unsupervised Pre-training**：用Auto-Encoder或者RBM的卷积形式一层一层地做无监督预训练, 最后加上分类层做有监督的Fine-Tuning。

**(4)、Transfer Learning（迁移学习）**：在某些情况下，训练集的收集可能非常困难或代价高昂。因此，有必要创造出某种高性能学习机（learner），使得它们能够基于从其他领域易于获得的数据上进行训练，并能够在对另一领域的数据进行预测时表现优异。

### 1a.优化的原理性问题

> 参考https://zhuanlan.zhihu.com/p/33173246

#### 1.为什么要进行normalization（标准化）

独立同分布（i.i.d）的数据集可以简化模型的训练提升模型的预测能力

白化（whitening）预处理就是为了使数据在进入模型之前，变为独立同分布，即消除特征之间的相关性，使所有特征具有相同的均值和方差

没有normalization的数据，在进行神经网络多层的叠加之后，每层的参数更新会导致上层的输入分布发生变化，在层层叠加之后，高层的输入分布会剧烈变化，使高层需要不断重新适应底层的参数更新，被google成为ICS，（神经网络的各层输出经过了层内的操作之后，分布显然与各层对应的输入不同，而这种不同会随着网络深度的增加而加大）ICS会导致每个神经元输入数据不再是独立同分布，上层参数需要不断适应新的输入数据分布，降低学习速率；下层的输入变化可能趋于变大或变小，导致上层落入饱和区，使学习过早停止；每层的更新都会影响其他层

所以需要在每层中放置一个白化操作，并且这个白化操作需要可微，来进行梯度反传

#### 2.Normalization通用框架

$$
h=f(g\cdot\frac{X-\mu}{\delta})+b)
$$

通用变换框架如上图$\mu$是**平移参数**（shift parameter）,$\delta$是**缩放参数**（scale parameter），通过这两个参数得到的
$$
\hat{X}=\frac{X-\mu}{\delta}
$$

其中$\hat{X}$符合0，1分布

而之后的g，b分别为再平移参数和在缩放参数

==为什么要再平移和缩放呢？==

第一步的变化将输入输出限制到了全局统一的范围（01分布），下层神经元努力学习，但不论其如何变化，传至上层前都会被粗暴的调整到这个固定范围，这叫白给，为了尊重底层的学习结果，进行第二次平移和缩放，使其变成b~ g*2*分布，这就使对于输入来说，可以针对该神经元形成特定的范围，但这种简单的白化不能完全意义上形成独立同分布，只是映射到了一个确定的区间范围

#### 3.主流Normalization方法

1. Batch Normalization（BN）

   可以看作一种纵向的规范化，针对单个维度定义，适用于数据分布比较接近的境况，BN不适用与动态网络结构和RNN网络

2. Layer Normalization（LN）

   横向规范化，针对BN不足提出，他综合考虑一层所有维度的输入，计算该层的平均数入职和输入方差，用同一个操作来转换各自维度的输入

   LN针对单个样本进行徐连，不依赖于其他数据，可以避免BN中受minibatch数据分布的影响，可以用于小nimibatch的场景，动态网络和RNN，尤其是自然语言处理领域此外，LN 不需要保存 mini-batch 的均值和方差，节省了额外的存储空间。

   但是，BN 的转换是针对单个神经元可训练的——不同神经元的输入经过再平移和再缩放后分布在不同的区间，而 LN 对于一整层的神经元训练得到同一个转换——所有的输入都在同一个区间范围内。如果不同输入特征不属于相似的类别（比如颜色和大小），**那么 LN 的处理可能会降低模型的表达能力**。意味着对CV可能没那么有用？

3. weight Normalization参数规范化

4. consine Normalization余弦规范化

#### 4.Normalization为什么有效

1. normalization权重伸缩不变性

   当权重W按照常量进行伸缩时，规范化后的值不变，归一化占比该是多少还是多少（因为是线性变化的原因）

   权重的伸缩不变性可以提高反向传播的效率，且还具有-参数正则化效果，可以使用更高的学习率，下层的权重值越大，其梯度就越小。这样，参数的变化就越稳定，相当于实现了参数正则化的效果，避免参数的大幅震荡，提高网络的泛化性能。

2.  Normalization 的数据伸缩不变性

   当数据X按照常量 进行伸缩时，得到的规范化后的值保持不变

   数据伸缩不变性可以有效地减少梯度弥散，简化对学习率的选择

### 2.损失函数

#### 0.正则化项

> 大部分内容来自 @阿拉丁吃米粉https://blog.csdn.net/jinping_shi/article/details/52433975

L1正则化，L2正则化，这两种正则化项都可以看作是损失函数的惩罚项，

- L1正则化是指权值向量w中各个元素绝对值之和，通常表示为$||w||_1$
- L2正则化是指权值向量w中各个元素的**平方和然后再求平方根**（可以看到Ridge回归的L2正则化项有平方符号），通常表示为$||w||_2$

一般都会在正则化项之前添加一个系数，Python的机器学习包`sklearn`中用α表示，一些文章也用λ表示。这个系数需要用户指定

L1L2正则化可以怎么用：

- L1正则化可以产生稀疏权值矩阵，即产生一个稀疏模型，可以用于特征选择
- L2正则化可以防止模型过拟合（overfitting）；一定程度上，L1也可以防止过拟合

##### (1) L1正则化

**L1正则化为什么可以产生稀疏模型**（所谓稀疏模型，就是矩阵中大部分值为0，少数元素非0，这样就可以吧重点放在非0 值上边，只关注这些特征，从而进行特征选择）

下为带L1正则化的损失函数：
$$
J = J_0+\alpha\sum|w|
$$
  J~0~ 为原始损失函数，$\alpha$是正则化系数，注意到L1正则化是权值的**绝对值之和**，J 是带有绝对值符号的函数，因此J是不完全可微的。要求$J$的最小值，如果在二维情况下，只有两个权值$w^1$和$w^2$，此时$J_0$可以画出等值线，

![@图1 L1正则化](https://imgconvert.csdnimg.cn/aHR0cDovL2ltZy5ibG9nLmNzZG4ubmV0LzIwMTYwOTA0MTg0NDI4NDU5?x-oss-process=image/format,png#pic_center)

图中等值线是$J_0$的等值线，黑色方形是L函数的图形。$L=|w^1|+|w^2|$ ，这个函数画出来就是一个方框。$J_0$和

L图形首次相交的地方就是最优解，L的图形在多为空间会有很多‘ 突出的角 ’，$J_0$与角接触的机会远大于和其他部分接触的机会，而在这些角上，就会有很多权值等于0的点，（在上图中$w_1$就是0值）所以L1正则化可以产生稀疏模型，进而可以用于特征选择

而正则化前面的系数α，可以控制L图形的大小，$\alpha$越大，L图形就越大

##### (2) L2正则化

带L2正则化的损失函数为：
$$
J = J_0+\alpha\sum w^2
$$
平面上二维图像为

<img src="https://imgconvert.csdnimg.cn/aHR0cDovL2ltZy5ibG9nLmNzZG4ubmV0LzIwMTYwOTA0MTg0NjQ2OTYz?x-oss-process=image/format,png#pic_center" alt="@图2 L2正则化"  />

L2正则化的函数图形是个院（绝对值的平方和是个圆），因此$J_0$与L相交时使得$w_1$或$w_2$等于0的概率就小了很多，所以L2正则化项不稀疏

#### 1.均方差

#### 2.交叉熵

#### 3.

### 3.激活函数

#### 1.sigmoid

<img src="https://images2015.cnblogs.com/blog/1042406/201702/1042406-20170224104856695-617608459.jpg" alt="img" style="zoom: 80%;" />

在配合==均方差==误差函数使用时，每一层向前递推都要乘以*σ*′(*z*)，得到梯度变化值。Sigmoid的这个曲线意味着在大多数时候，我们的梯度变化值很小，导致我们的W，b更新到极值的速度较慢，也就是我们的算法收敛速度较慢。

所以搭配交叉熵损失函数比均方差效果好

#### 2.softmax

#### 3.ReLU

解决梯度消失问题

#### 4.GeLU

### 3.神经网络、NLP相关

#### 1.DNN

> 工业中应用很多

含有多隐藏层的神经网络、多层感知机，感知机的叠加，

输入层、隐藏层、输出层

![img](https://images2015.cnblogs.com/blog/1042406/201702/1042406-20170220122136538-2002639053.png)

反向传播算法（BP），对于神经网络来说，需要对W，b进行一系列的修正，也就是我们所说的算法的训练，需要对算法的结果和训练集的正确结果计算损失函数（具体的损失函数需要自己选择）之后，根据损失函数进行优化，即求得损失函数的极小值（or最小值、每层的梯度方向进行优化）（让损失最小），也就是我们说的机器学习优化问题，每一层的梯度都需要根据上一层求得

为了防止过拟合，还需要加入正则化，参考2.0中正则化的部分

正则化：

> https://www.zhihu.com/question/20924039

#### 2.CNN 卷积神经网络

对于NLP任务来说，CNN没有序列依赖问题

#### 3.RNN 循环神经网络

给定一个一句话前面的部分，预测接下来最有可能的一个词是什么。

有梯度消失和梯度爆炸问题，但梯度消失不太好解决，梯度爆炸可以限制阈值来解决。

==缺点== ：难以处理长距离依赖，线性序列结构导致反向传播优化有很多困难，导致梯度消失的问题

> e.g  如果有句话主语和谓语中间隔了很长的从句，RNN就无法处理主语谓语关系，长距离使得RNN可用性变差

#### 4.LSTM（长短时记忆网络）（RNN优化）

用来避免长距离依赖

对比于RNN，增加了一个状态c，让他来保存长期的状态，

![LSTM概念图](https://img-blog.csdn.net/20180308104744375?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbG92ZV9fbGl2ZTE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

h~t~为该时刻的LSTM输出值，c~t~为该单元的单元状态， x~t~为对应的网络输入值

> LSTM的关键，就是怎样控制长期状态c。在这里，LSTM的思路是使用三个控制开关。第一个开关，负责控制继续保存长期状态c；第二个开关，负责控制把即时状态输入到长期状态c；第三个开关，负责控制是否把长期状态c作为当前的LSTM的输出。

实现开关的算法，就是门，即LSTM有遗忘门，输入门和输出门

​	所谓的门，可以表示为
$$
g(x) = σ(Wx + b)
$$
​	σ为激活函数（0，1）， b为偏置量，



LSTM用两个门控制c，分别为遗忘门和输入门

遗忘门：决定了上一时刻的单元状态c~t−1~有多少保留到当前时刻c~t~
$$
f_t=σ(W_f\times[h_t−1,x_t]+b_f)
$$
![img](https://img-blog.csdn.net/20180308104757785?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbG92ZV9fbGl2ZTE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)



输入门：决定了当前时刻网络的输入x~t~有多少保存到单元状态c~t~
$$
i_t=σ(Wi\times[h_{t−1},x_t]+b_i)
$$
![img](https://img-blog.csdn.net/20180308104809315?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbG92ZV9fbGl2ZTE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

当前输入状态c~t~
$$
\overline{c_t}=tanh(W_c[h_{t−1},x_t]+b_c)
$$
![img](https://img-blog.csdn.net/20180308104828897?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbG92ZV9fbGl2ZTE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

当前时刻的单元状态c~t~为(按元素乘)
$$
c_t = f_t\times{c_{t-1}}+i_t\times{\overline{c_t}}
$$
![img](https://img-blog.csdn.net/20180308104837579?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbG92ZV9fbGl2ZTE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

输出门o~t~
$$
 o_t=σ(W_o[h_{t−1},x_t]+b_o)
$$
![img](https://img-blog.csdn.net/20180308104849675?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbG92ZV9fbGl2ZTE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

 从而最终输出为
$$
h_t=o_t \times tanh(c_t)
$$
![img](https://img-blog.csdn.net/20180308104857642?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbG92ZV9fbGl2ZTE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

#### 5.seq2seq

encoder+decoder模型，输入一个序列，输出一个序列

### 4.机器学习算法

> 部分内容来自 知乎@月亮满了 https://www.zhihu.com/question/26726794/answer/1494975658

#### 1.逻辑回归（LR）

> 不要瞧不起LR，工程上用的多，诸如银行反欺诈系统，用GBDT算法+LR，因为LR特征可以解释，特征工程清晰，在银行这种强监管的场景下，深度学习这种不可解释的模型不符合要求

将线性回归预测出来的数据映射到logit函数上，完成（0，1）的事件概率预测

> logit函数是：$log(\frac{P}{1-P})=\beta_0+\beta_1x_1$，对应图像为
>
> <img src="http://www.appstate.edu/~whiteheadjc/service/logit/logit.gif" alt="查看源图像" style="zoom:70%;" />

LR是很多分类算法的组件，他的输出值自然的落在0，1上，可以用以分析单一因素对某个时间发生的影响因素分析，

优点：

- 从整体模型来说，模型清洗，背后的概率推导经得住推敲；
- 从输出值来说，输出值自然落在0到1之间，并且有概率意义；
- 从模型参数来说，参数代表每个特征对输出的影响，可解释性强；
- 从运行速度来说，实施简单，非常高效（计算量小、存储占用低），可以在大数据场景中使用；
- 从过拟合角度来说，**解决过拟合的方法很多，如L1、L2正则化**；
- 从多重共线性来说，L2正则化就可以解决多重共线性问题；

缺点：

- （特征相关情况）因为它本质上是一个线性的分类器，所以处理**不**好**特征之间相关**的情况；
- （特征空间）特征空间很大时，性能不好；
- （预测精度）容易**欠拟合**，预测精度不高；

#### 2.树模型

决策树模型是运用于分类以及回归的一种树结构。决策树由节点和有向边组成，待测数据与决策树中的特征节点进行比较，并按照比较结果选择下一比较分支，直至叶节点作为最终的决策结果

树模型 可以生成清晰的**基于特征选择**不同预测结构的树状结构，常见于**垃圾邮件躲避检测**中，但由于决策树在底层上的判断基于条件，攻击者可以修改几个检测条件就躲避检测

决策树往往是一些算法的基石，原理很简单

优点：

- 容易理解、可读性强，比较直观；
- 自变量/特征可以是连续变量，也可以是分类变量；
- 可处理缺失值；
- 基本不用做原始数据的预处理，如标准化等；
- 可以建立非线性模型；
- 即使是较大的数据及，其训练时间也很短；

劣势：

- 大型的决策树较难解释；
- 方差大的决策树会导致模型表现较差；
- 容易出现过拟合；

#### 3.集成模型

通过将多个弱学习器组合成一个强分类器，泛化能力强。主要为Bagging、Boosting

基于Bagging：随机森林，

基于Boosting：Adaboost、BGDT、XgBoost、LightGBM

Bagging和Boosting的区别总结如下：

- **在样本选择上：**Bagging方法的训练集是从原始集中有放回的选取，所以原始集中选出的各轮训练集之间是独立的；而Boosting方法需要每一轮的训练集**不变**，知识训练集中每个样本在分类器中的权重发生变化。而权值是根据上一轮的分类结果进行调整；
- **在样例权重上：**Bagging方法使用均匀取样，所以每个样本的权重相等；而Boosting方法根据错误率不断调整样本的权重，错误率越大则权重越大；
- 在预测函数上：Bagging方法中所有预测函数的权重相等；而Boosting方法汇总每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重；
- 并行计算上：Bagging方法中各个预测函数可以并行生成；而Boosting方法各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果；

##### 3.1随机森林

随机选取**不同的特征和训练样本**，生成大量的决策树，然后综合这些决策树的结果来进行最终的分类。

随机森林在现实分析中被大量使用，相对于决策树，在准确性有了很大的额提升，同时一定程度上改善了决策树容易被攻击的特点。

 优点：

- 在所有的算法中，随机森林具有极好的准确率；
- 能够运行在大数据集上；
- 能够处理具有高维特征的输入样本，**而且不需要降维；**
- 能够评估各个特征在分类问题上的重要性；
- 在生成过程中，能够获取内部生成误差的一种无偏估计；
- 对于缺失值也能够获得很好的结果；

缺点：

- 据观测，如果一些分类/回归问题的训练数据中存在噪音，随机森林中的数据集中会出现**过拟合**的现象；
- 相对决策树来说，随机森林算法更复杂，计算成本更高（因为RF是有多个决策树组成）；
- 由于其本身的复杂性，它们比其他类似的算法需要更多的时间来训练；

**适用情景：**

数据维度相对低（几十维），同时对准确性有较高要求时。

因为不需要很多参数调整就可以达到不错的效果，基本上不知道用什么方法的时候都可以先试一下随机森林。

##### 3.2基于Boosting思想的集成模型

**XGBoost**

梯度提升机器算法，其基本思想是把成百上千个分类准确率较低的树模型组合成一个准确率较高的模型，该模型不断迭代，每次迭代生成一颗新的树，如何在每一步生成合理的树是Boosting分类器的核心。



#### 4.SVM 支持向量机

SVM的核心思想就是找到不同类别之间的分界面，使得两类样本尽量落在面的两边，而且离分界面尽量远。

最早的SVM是平面的，局限很大。但是利用核函数(kernel function)，我们可以把平面投射(mapping)成曲面，进而大大提高SVM的适用范围。

厉害是因为核函数，线性不可分的数据，放在高纬度上，可能就线性可分了

核函数可以使我们不需要知道低维到高维的映射函数，直接在低维进行点积运算，直接映射到高维

**核函数**

linear、poly、rbf、sigmoid

其中最常使用的使高斯核rbf，

**适用情景：**

SVM在很多数据集上都有优秀的表现。

相对来说，SVM尽量保持与样本间距离的性质导致它抗攻击的能力更强。

和随机森林一样，这也是一个拿到数据就可以先尝试一下的算法。

## 2.查找算法

### 1.倒排索引

> 今日头条推荐算法应用此算法，离线维护一个倒排，倒排的key可以分类，topic、实体、来源等，排序可以考虑热度、新鲜度、动作。线上召回可以迅速从倒排中更具用户标签对内容做截断，从而筛选小部分内容

在关系型数据库里，倒排索引是最有效率的方式

在正常数据库中，是以文档ID作为索引，以文档内容为记录，但倒排索引是以单词作为索引，文档作为记录，这样可以方便的通过单词或记录查找到其所在文档

## 3.数据问题

### 1.传统文本的数据增强（NLP）

> 知乎 清风拂岗 明月映江 https://zhuanlan.zhihu.com/p/75207641?utm_source=wechat_session

对于NLP的数据增强大概有两种方案

#### 1.加噪声

（EDA easy data Augmentationg for text classification tasks）

- 同义词替换：不考虑同义词问题，在句子中抽取n个词，替换为同义词

- 随机插入：随机抽取一个词，然后在该词的同义词集合中随机选择一个，插入原句子中的随机位置。该过程可以重复n次

  Eg : “**我非常喜欢这部电影**” —> “**爱我非常喜欢这部影片**”

- 随机交换：随机选择句子中的两个词，交换位置

- 随机删除：对于句子中的每个词，以概率p随机删除

缺点：

**同义词替换SR**有一个小问题，同义词具有非常相似的词向量，而训练模型时这两个句子会被当作几乎相同的句子，而在实际上并没有对数据集进行有效的扩充。**随机插入RI**很直观的可以可以看到原本的训练数据丧失了语义结构和语义顺序，而不考虑停用词的做法使得扩充出来的数据并没有包含太多有价值的信息，同义词的加入并没有侧重句子中的关键词，在数据扩充的多样性上实际会受限较多。**随机交换RS** 实质上并没有改变原句的词素，对新句式、句型、相似词的泛化能力实质上提升很有限。**随机删除RD**不仅有随机插入的关键词没有侧重的缺点，也有随机交换句式句型泛化效果差的问题。随机的方法固然能够照顾到每一个词，但是没有关键词的侧重，若随机删除的词刚好是分类时特征最强的词，那么不仅语义信息可能被改变，标签的正确性也会存在问题。

#### 2.回译

将文本翻译为另一种语言，然后翻译回来

但可以想象，回译必须依赖翻译的质量，且工作量较大，需要翻译软件的接口

### 2.深度学习的数据增强

#### 1.半监督 mixmatch

更好地利用未标注的数据，减轻对于大规模标注数据集的依赖；**Mixup 增广术**

MixMatch的工作方式是通过 MixUp 猜测数据扩增方法产生的无标签样本的低熵标签，并把无标签数据和有标签数据混合起来。只有 250 个标签的情况下，作者们把错误率降低到了之前方法的 1/4。

可以具体去参考论文

#### 2.无监督数据增强

UDA unsupervised data augmentation

MixMatch 算法除了使用普通的数据增广，还有一个秘诀是 **Mixup 增广术**。而 UDA 的成功，得益于对特定任务使用**特定目标的数据增强算法**。这种方法能够产生有效、真实的噪声，且噪音多样化。另外以目标和性能为导向的数据增强策略可以学习如何在原始标记集中找出丢失的或最想要的训练信号。（比如图像数据以颜色为目标进行数据增强）

### 3.其他类型的数据增强

#### 1.音频：

- 噪声增强
- 随机相同类型抽取拼接
- 时移增强
- 音高变换增强
- 速度调整
- 音量调整
- 混合背景音
- 增加白噪声
- 移动音频
- 拉伸音频信号

#### 2.图像：

- 水平翻转垂直翻转
- 旋转
- 缩放 放大缩小
- 裁剪
- 平移
- 高斯噪声
- 生成对抗网络 GAN
- AutoAugment

#### 3.文本其他数据增强方法：

- 语法树结构替换
- 篇章截取
- seq2seq序列生成数据
- 生成对抗网络 GAN
- 预训练的语言模型